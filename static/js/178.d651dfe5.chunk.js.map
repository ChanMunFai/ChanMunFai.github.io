{"version":3,"file":"static/js/178.d651dfe5.chunk.js","mappings":"yMAmFA,EAjFuB,WACrB,OACEA,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,WACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACED,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EAAIC,EAAAA,EAAAA,KAAA,KAAGC,KAAK,oBAAmBF,SAAC,UAAS,QACzCD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EAAIC,EAAAA,EAAAA,KAAA,KAAGC,KAAK,6CAA4CF,SAAC,SAAQ,WAEnED,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,aACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,2DACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,CAAI,6QAIFD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,6LAGJC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,sEAOZD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,cACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,kvBAcLD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,kDACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,qYAQHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,8uBAYHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,+qBAcX,ECIA,EAnFsB,WACpB,OACED,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,iBACJD,EAAAA,EAAAA,MAAA,KAAAC,SAAA,CAAG,6BAAyBC,EAAAA,EAAAA,KAAA,UAAAD,SAAQ,cAAkB,oNACtDC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,WACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACED,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EAAIC,EAAAA,EAAAA,KAAA,KAAGC,KAAK,0CAAyCF,SAAC,UAAS,QAC/DD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EAAIC,EAAAA,EAAAA,KAAA,KAAGC,KAAK,mDAAkDF,SAAC,SAAQ,WAEzED,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,aACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,kFACJC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,gHAGRD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,cACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,m6BAgBLD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,kDACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,oTAMHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,sSAMHD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACED,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,UAAAD,SAAQ,kBAAsB,2GAG9BC,EAAAA,EAAAA,KAAA,MAAAD,UACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,2QAQRD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,UAAAD,SAAQ,iBAAqB,yQAMjCC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,2YAWX,ECYA,EA7FkB,WAChB,OACED,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,WACJC,EAAAA,EAAAA,KAAA,MAAAD,UACED,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EAAIC,EAAAA,EAAAA,KAAA,KAAGC,KAAK,2EAA0EF,SAAC,UAAS,UAElGD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,aACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,iCACJC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,6DACJD,EAAAA,EAAAA,MAAA,MAAAC,SAAA,CAAI,uCACiCC,EAAAA,EAAAA,KAAA,UAAAD,SAAQ,mBAAuB,sLAGpEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,oNAMRD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,cACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,+qCAkBLD,EAAAA,EAAAA,MAAA,OAAAC,SAAA,EACEC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,kDACJC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,4ZAQHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,ylBASHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,wgBAQHC,EAAAA,EAAAA,KAAA,KAAAD,SAAG,yfAOHD,EAAAA,EAAAA,MAAA,KAAAC,SAAA,CAAG,4DAEDC,EAAAA,EAAAA,KAAA,MAAAD,SAAI,2GACqD,8eAYnE,ECjDA,EApCoB,SAAHG,GAAkB,IAAZC,EAAID,EAAJC,KACfC,GAASC,EAAAA,EAAAA,MACTC,EAAOH,EAAKI,MAAK,SAACC,GAAQ,OAAKA,EAASC,KAAOL,EAAOK,EAAE,IAE1DC,EAAY,KAchB,OAZEA,EADc,MAAZJ,EAAKG,GACKE,EAEO,MAAZL,EAAKG,GACAG,EAEO,MAAZN,EAAKG,GACAI,EAGA,MAIZb,EAAAA,EAAAA,KAACc,EAAAA,EAAI,CACHC,MAAM,WACNC,YAAY,qBAAoBjB,UAEhCD,EAAAA,EAAAA,MAAA,WAASmB,UAAU,OAAMlB,SAAA,EACvBC,EAAAA,EAAAA,KAAA,UAAAD,UACED,EAAAA,EAAAA,MAAA,OAAKmB,UAAU,QAAOlB,SAAA,EACpBC,EAAAA,EAAAA,KAAA,MAAAD,SAAKO,EAAKS,SACVf,EAAAA,EAAAA,KAAA,MAAAD,SAAKO,EAAKY,gBAGbR,IAAaV,EAAAA,EAAAA,KAACU,EAAS,QAIhC,C","sources":["data/projects/imperial_thesis.js","data/projects/imperial_group.js","data/projects/lse_thesis.js","components/ProjectPost/ProjectPost.js"],"sourcesContent":["import React from 'react';\n\nconst ImperialThesis = () => {\n  return (\n    <div>\n      <h4>Links</h4>\n      <ul> \n        <li><a href=\"http://google.com\">Paper</a> </li>\n        <li><a href=\"https://github.com/ChanMunFai/Dissertation\">Repo</a> </li>\n      </ul>\n      <div> \n        <h2>Summary</h2>\n        <ul>\n          <li> Attained Distinction of 76% for Masters Dissertation</li>\n          <li> \n            Combined Deep Learning (Variational Auto-Encoders) with classical linear dynamical models (Kalman Filters), \n            allowing for more efficient model with improved performance and ability to separate dynamics from appearance \n            and disentangle different dimensions of dynamics \n            <ul> \n              <li>e.g. for a bouncing ball, model is able to process the ball's appearance and its movement separately.\n                For its movement, model is able to separate its horizontal and vertical movement.\n              </li>\n              <li> \n                enables better generalisability and interpretability \n              </li>\n            </ul>\n          </li>\n        </ul>\n      </div>\n      <div>\n        <h2>Abstract</h2>\n        <p>\n          Long-term video prediction remains a highly challenging problem. \n          The high dimensionality of pixels often warrants for highly complex \n          models that are difficult and expensive to train. I build upon the \n          work of the Kalman Variatonal Autoencoder (KVAE), a highly efficient \n          model that combines variational autoencoders (VAEs) with the Linear \n          Gaussian State Space Model. In doing so, it is able to effectively \n          disentangle an objectâ€™s representation from its dynamics. In this \n          paper, I reformulate the Evidence Lower Bound of the KVAE to develop\n          a hierarchy of latent variables for the KVAE. The hierarchical KVAE \n          can outperform the KVAE on certain datasets and decompose latent \n          dynamics according to its levels, enabling greater interpretability.\n        </p>\n      </div>\n      <div>\n        <h2>Please explain to me what all of this means!</h2>\n        <p>\n          My project deals with video prediction, which is the problem of \n          predicting the next few frames of a video given the first few frames.\n          You can imagine this to be useful for autonomous driving and robotics that\n          are controlled by AI. However, models in this field tend to be highly\n          complex (with the consequence being that they are too large to run on computers and \n          expensive to train).\n        </p>\n        <p>\n          I attempt to circumvent this problem by predicting a lower-dimensional variable \n          instead of pure pixels in a video frame, which makes this problem significantly simpler. \n          The difficulty then lies in finding this 'lower-dimensional variable', which should contain\n          sufficient information so that prediction is still accurate, whilst not containing too much \n          information such that we require complex solutions again. \n          One such method is the Kalman Variational AutoEncoder(KVAE) model. \n          Without diving into technicalities, this model processes\n          an object's appearance and movement separately. Hence, if the object\n          looks largely the same throughout the video, being able to predict its\n          movement is sufficient in predicting how it appears in future frames.\n        </p>\n        <p>\n          I further improve this method by incorporating hierarchies in my model. \n          Intuitively, this allows us to then process movements operating at different speeds\n          more efficiently because we can program these hierachies to operate at various\n          time scales. \n          In terms of the process, this project required advanced mathematics and statistics, \n          as I spent most of my time with a pen and paper deriving the mathematical foundations\n          behind my newly developed model. Please feel free to read my paper if you are interested\n          in that! After implementing it on paper, the next challenge will be to program it in an efficient\n          manner in Pytorch and be able to test and perform hyperparameter tuning.\n        </p>\n      </div>\n    </div>\n  );\n};\n\nexport default ImperialThesis;","import React from 'react';\n\nconst ImperialGroup = () => {\n  return (\n    <div>\n      <h4>Publication</h4>\n      <p>Anton, J.; Castelli, L.; <strong>Chan, M.F</strong>.; Outters, M.; Tang, W.H.; Cheung, V.; Shukla, P.; Walambe, R.; Kotecha, K. How Well Do Self-Supervised Models Transfer to Medical Imaging? J. Imaging 2022, 8, 320. https://doi.org/10.3390/jimaging8120320</p>\n      <h4>Links</h4>\n      <ul> \n        <li><a href=\"https://www.mdpi.com/2313-433X/8/12/320\">Paper</a> </li>\n        <li><a href=\"https://github.com/jonahanton/SSL_medicalimaging\">Repo</a> </li>\n      </ul>\n      <div> \n        <h2>Summary</h2>\n        <ul>\n          <li> Published paper in MDPI Journal of Imaging as part of Masters group project</li>\n          <li> Won best poster award (500 pounds) out of 40 PHD teams in Imperial College London research showcase</li>\n        </ul>\n      </div>\n      <div>\n        <h2>Abstract</h2>\n        <p>\n        Self-supervised learning approaches have seen success transferring \n        between similar medical imaging datasets, however there has been no \n        large scale attempt to compare the transferability of self-supervised \n        models against each other on medical images. In this study, we compare\n         the generalisability of seven self-supervised models, two of which were \n         trained in-domain, against supervised baselines across eight different \n         medical datasets. We find that ImageNet pretrained self-supervised models \n         are more generalisable than their supervised counterparts, scoring up to \n         10% better on medical classification tasks. The two in-domain pretrained \n         models outperformed other models by over 20% on in-domain tasks, however \n         they suffered significant loss of accuracy on all other tasks. Our \n         investigation of the feature representations suggests that this trend \n         may be due to the models learning to focus too heavily on specific areas.\n        </p>\n      </div>\n      <div>\n        <h2>Please explain to me what all of this means!</h2>\n        <p>\n          Most advances in Machine Learning has been in the field of supervised \n          learning, where data points come with their corresponding labels which the \n          model can use to evaluate its own performance. E.g. a chest X-ray dataset\n          will contain not only images, but labels indicating the types of diseases depicted.\n        </p>\n        <p>\n          However, the cost of annotating labels is expensive, especially in certain\n          niche contexts like medical imaging. On the other hand, there is an abundance\n          of unlabelled data which we should leverage on. One such solution is self-supervised\n          learning (SSL). Typically, this involves 2 stages. \n        </p>\n        <ol>\n          <li>\n            <strong>Pretraining: </strong> \n            &nbsp; Here, the model can only access unlabelled data. It learns structural patterns\n            within the dataset. \n            <ul>\n              <li>\n                E.g. one popular SSL technique is to give a model an image and an augmented\n                version of the first original image. It is also fed other images, with the goal of \n                learning that the original and augmented image are the same, and different from the\n                other images. \n              </li>  \n            </ul> \n          </li>\n          <li>\n            <strong>Finetuning: </strong> \n            &nbsp; The model is then finetuned on the specific domain we are interested in (and\n            that does not have to be in exactly the same domain as the pretraining phase). If this is\n            a supervised learning problem, our finetuning stage needs to be supervised as well. \n          </li>\n        </ol>\n        <p>\n          In practice, it is too expensive (as it takes weeks) to pretrain \n          one own SSL model from scratch. Very often, one finds a readily available\n          pretrained model and then finetunes it on his/her own purposes, even when \n          the pretraining and finetuning domain may not be exactly the same.\n          In light of this, this paper investigates how well pretrained SSL methods can \n          be used in medical imaging. \n        </p>\n      </div>\n    </div>\n  );\n};\n\nexport default ImperialGroup;","import React from 'react';\n\nconst LseThesis = () => {\n  return (\n    <div>\n      <h4>Links</h4>\n      <ul> \n        <li><a href=\"https://drive.google.com/drive/folders/17xfvU9fRo_eDXHVSFtE_s6bcrXoKf2Iw\">Paper</a> </li>\n      </ul>\n      <div> \n        <h2>Summary</h2>\n        <ul>\n          <li>Best dissertation in cohort</li>\n          <li>Selected to present at LSE Interdisciplinary Conference</li>\n          <li>\n            Employed novel causal ML technique <strong> (Generic ML) </strong>to estimate hetergeneous treatment effects in Randomised Controlled Trials(RCTs). \n            This enabled me to identify flaws in past RCTs analysed using traditional linear regressions. \n          </li>\n          <li>\n            Wrote code for algorithm from scratch in Python. Over 1000% speed boost as compared to original R code.\n            Faster code enabled for Monte Carlo simulations performed on High Performance Computing cluster.\n          </li>\n        </ul>\n      </div>\n      <div>\n        <h2>Abstract</h2>\n        <p>\n        I build on the work of Generic Machine Learning Inference, a Machine\n        Learning(ML) approach to estimate heterogeneous treatment effects in RCTs. Through\n        Monte Carlo simulation, I investigate the performance of the estimator under different\n        data generating processes (DGPs) and demonstrate its ability to make valid inference\n        under different settings. Specifically, I show that Generic ML - which requires no\n        parametric assumption - is significantly more powerful than classic heterogeneity tests\n        under functional form mis-specification, a plausible situation given that researchers do\n        not generally have access to the true DGP. I also improve its implementation by re-\n        proposing performance metrics that identify the best ML method. Applying Generic\n        ML to an RCT of village-based schools in Afghanistan, I derive new insights and thus\n        demonstrate the usefulness of this approach. Specifically, I find that treatment effects\n        on school enrollment and test scores increase with age and distance to non village-\n        based schools for girls. This proved particularly insightful as distance was raised as\n        a potential explanation for heterogeneity but not formally analysed by the original\n        researchers.\n        </p>\n      </div>\n      <div>\n        <h2>Please explain to me what all of this means!</h2>\n        <p>\n          Econometrics (analysis of data in economics) and Machine Learning (ML)\n          tend to have slightly different goals; the former tends to be more concerned\n          with causality whilst the latter is concerned with correlation. Additionally, \n          traditional econometrics often use more restrictive and less powerful models - e.g. \n          linear regression or also known as Ordinary Least Squares(OLS) - to better aid \n          interpretability.\n        </p>\n        <p>\n          An example of a model that intersects between econometrics and ML is that of \n          Generic ML. For our purposes, we can just think of it as a fancy algorithm that uses ML\n          but can still infer causality. Specifically, it is used to infer heterogeneous treatment \n          effects in RCTs (experiments), i.e. it can estimate how different subgroups are affected \n          differently by the same treatment. Generally, this is something that OLS struggles with, \n          because it tends to lose power (in a non-technical sense, the model is unable to identify\n          differences in treatment effects when there are indeed such differences).\n        </p>\n        <p>\n          When I was doing my dissertation, the field and this particular technique was very nascent, \n          with no other papers based on this technique. Hence, I had to first evaluate if this technique \n          is even suitable, and the conditions under which it will work well. I also had to rewrite the code\n          in Python which is much faster than R. I generated fake data\n          with pre-specified treatment effects and tested if the technique can identify these effects.\n          Repeating this thousands of times is also known as Monte Carlo simulation. \n        </p>\n        <p>\n          The primary goal of this paper is to identify if our new technique (Generic ML) can do better\n          than traditional OLS. I re-analysed an RCT study that investigated the effects of building a school \n          on school children in rural Afghanistan. The original authors found that older girls benefitted more\n          from such a program. Their hypothesis is that as girls grow older, they will require a chaperone to accompany \n          them to school. Hence, in the absence of this program, older girls simply do not go to school. \n        </p>\n        <p>\n          Intuitively, for this hypothesis to work, we also expect  \n          <em> older girls who live further away from the nearest \n            pre-teatment school to benefit disproportionately </em>\n          as well since it is even harder for them to make the journey to school. \n          Puzzlingly, the researchers did not test if distance created \n          differing treatment effects. I extended their OLS study and found no significant results,\n          which may imply a lack of power associated with OLS. Replicatiing the entire study\n          using Generic ML then found evidence that older girls who live further away\n          benefit more, thus lending greater support to the hypothesis that the \n          researchers initially laid out. \n        </p>\n      </div>\n    </div>\n  );\n};\n\nexport default LseThesis;","import React, { useState } from 'react';\nimport Main from '../../layouts/Main';\nimport { useParams } from 'react-router';\n\nimport ImperialThesis from '../../data/projects/imperial_thesis'\nimport ImperialGroup from '../../data/projects/imperial_group'\nimport LseThesis from '../../data/projects/lse_thesis'\n\nconst ProjectPost = ({ data }) => {\n  const params = useParams();\n  const post = data.find((dataItem) => dataItem.id === params.id);\n  \n  let Component = null;\n  if (post.id === '1') {\n    Component = ImperialThesis;\n  } \n  else if (post.id === '2') {\n    Component = ImperialGroup;\n  }\n  else if (post.id === '3') {\n    Component = LseThesis;\n  }\n  else {\n    Component = null;\n  }\n  \n  return (\n    <Main \n      title=\"Projects\" \n      description=\"Mun Fai's Projects\"\n    >\n      <article className=\"post\">\n        <header>\n          <div className=\"title\">\n            <h2>{post.title}</h2>\n            <h3>{post.subtitle}</h3>\n          </div> \n        </header>\n        {Component && <Component />}\n      </article>\n    </Main>\n  );\n};\n\nexport default ProjectPost;\n"],"names":["_jsxs","children","_jsx","href","_ref","data","params","useParams","post","find","dataItem","id","Component","ImperialThesis","ImperialGroup","LseThesis","Main","title","description","className","subtitle"],"sourceRoot":""}