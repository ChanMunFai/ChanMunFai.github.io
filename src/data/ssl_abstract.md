 <ul>
    <li>Best poster award.</li>
</ul>

#### Abstract

> Self-supervised learning approaches have seen success transferring
within domain for medical imaging, however there has been no large
scale attempt to compare the transferability of self-supervised mod-
els against each other on medical images. In this study, we compare
the generalisability of seven self-supervised models, two of which were trained in-domain, against supervised baselines across nine different medical datasets. We find that ImageNet pretrained self-supervised models are more generalisable than their supervised counterparts, scoring up to 10% better on medical classification tasks. The two in-domain pretrained models outperformed other models by over 20% on in-domain tasks, however suffered significant loss of accuracy on all other tasks. Our investigation of the feature representations suggests that this trend may be due to the models learning to focus too heavily on specific areas.